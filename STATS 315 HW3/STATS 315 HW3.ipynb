{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAUtuvQ-YPWc"
      },
      "source": [
        "<font color=\"#de3023\"><h1><b>REMINDER: MAKE A COPY OF THIS NOTEBOOK, DO NOT EDIT</b></h1></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d89fHnC4YR5a"
      },
      "source": [
        "Homework 3: In this homework, you will train a two layer neural network using gradient descent. However, instead of manually computing the gradients, you will use the autodiff provided by Tensorflow package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MLZbUp8aYfE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ6iFPm0Zn8U"
      },
      "source": [
        "# Two-layer Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPPcL-_mCZgZ"
      },
      "source": [
        "\n",
        "A two layer neural network contains an input layer, a hidden layer, and an output layer. The number of the input nodes in the diagram below is determined the dimension of our features. We are free to choose the dimension of the hidden layer. As for the final output layer, the number of nodes is determined by the type of the problem we are doing. For instance, for a regression problem, we will only have one node and the output value corresponds to our prediction of the target. As for classification, we will first output a vector that has same number of dimension as the number of classes in our classification dataset. Then, we will apply the softmax transformation on the vector to transform real-valued predictions to the class probabilities. \n",
        "![](https://www.researchgate.net/profile/Haohan-Wang-4/publication/282997080/figure/fig4/AS:305939199610886@1449952997594/A-typical-two-layer-neural-network-Input-layer-does-not-count-as-the-number-of-layers-of.png)\n",
        "\n",
        "Mathematically, this model can be written as \n",
        "\n",
        "$$f(x) =  \\sigma(x^{\\intercal} W_1  +b_1) W_2 + b_2. $$\n",
        "\n",
        "Note that if $x \\in \\mathbb{R}^{d \\times 1}$, then $W_1 \\in \\mathbb{R}^{d \\times H}$ and $W_2 \\in \\mathbb{R}^{H \\times O}$, where $O$ is the output dimension. The dimension of $b_1$ and $b_2$ is self-evident.\n",
        "\n",
        "Given an input $x$, the model first transforms it using the weight matrix $W_1$ and subsequently shifts the output by the bias term $b_1$. The function $\\sigma(.)$ is called the activation function that introdues non-linearity in the model. For the purpose of this homework, we will use the so called relu-activation function that is defined as $\\sigma(t) = \\text{max}\\{t, 0\\}$. Note that that $x^{\\intercal} W_1  +b_1$ generally gives us a vector, so we have to apply the relu activation to each element of the vector. Following the activation, the vector $h(x) =\\sigma(x^{\\intercal} W_1  +b_1)$ is defines the hidden layer. Finally, we apply the linear trasformation $ h(x) W_1 + b_2$ on the hidden layer. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFuaUzk4WPco"
      },
      "source": [
        "This representation of the network is very convenient if you instead want to do matrix operations on your data. Suppose $X$ be your data matrix where $i^{th}$ row of $X$ contains $x_i^{\\intercal}$, then the output of the network on the entire dataset can be written as \n",
        "$$f(X) = \\sigma(X W_1  +b_1) W_2 + b_2. $$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKRxewtuYnJ1"
      },
      "source": [
        "# Regression "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRXIXnYXGM_t"
      },
      "source": [
        "In homework 2, you trained a linear regression model on california housing dataset. In this homework, you will train a two-layer neural network on this dataset using gradient descent. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqhXKxB8equj"
      },
      "outputs": [],
      "source": [
        "# load the dataset\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "california_housing = fetch_california_housing( return_X_y=True, as_frame=True)\n",
        "X = california_housing[0]\n",
        "y = california_housing[1]\n",
        "X_train_unscaled, X_test_unscaled, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "sc=StandardScaler()\n",
        "X_train=sc.fit_transform(X_train_unscaled)\n",
        "X_test = sc.transform(X_test_unscaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEjywXczhLNv"
      },
      "outputs": [],
      "source": [
        "# convert numpy arrays to tensors\n",
        "\n",
        "X_train = tf.convert_to_tensor(X_train, dtype = tf.float32)\n",
        "X_test = tf.convert_to_tensor(X_test, dtype = tf.float32)\n",
        "y_train = tf.convert_to_tensor(y_train, dtype = tf.float32)\n",
        "y_test= tf.convert_to_tensor(y_test, dtype = tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2xrmNPqHUvL"
      },
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 1: Fill in the input dimension and output dimension of the two layer neural network for regression on this dataset. (5 pts) </b></h6></font> \n",
        "For the purpose of this homework, we will just choose hidden layer with dimension double that of input dimension. However, going forward choosing the hidden dimension appropriately would be an important part of deep learning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF_mIPodILR6"
      },
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 2: Define a tensorflow variables for weights W1, b1, W2, and b2. Then, initialize both biases b1 and b2 to be 0 vectors and initialize W1 and W2 by picking values uniformly at random from the interval [0,1]. (5 pts) </b></h6></font> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdYUj6S4Hddy",
        "outputId": "cece192e-cc98-4955-b7fd-99c7b56cb4c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([1, 16])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "b1.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuGkzh54JC3K"
      },
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 3: Complete the model function below to define a two layer neural network. Here, inputs is a matrix of shape $n \\times d$, where $i^{th}$ row of the inputs matrix contains $x_i^{\\intercal}$. (10 pts) </b></h6></font> \n",
        "Hint: Use [tf.nn.relu()](https://www.tensorflow.org/api_docs/python/tf/nn/relu) function for relu activation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzj0hUiiK_DG"
      },
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 4: Write a function below that computes mean-squared error given the predictions and targets. You should use tensorflow operations like tf.square and tf.reduce_mean for autodiff to work.  (10 pts) </b></h6></font> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uSkG_ixcLWl"
      },
      "outputs": [],
      "source": [
        "def mse(predictions, targets):\n",
        "  squared_error = tf.square(predictions-targets)\n",
        "  return tf.reduce_mean(squared_error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0NCJNXELmK-"
      },
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 5: Complete the function below that takes in features and targets and trains your two layer neural network using gradient descent. Please use autodiff by Gradient taping.  (10 pts) </b></h6></font> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCI8zqrL_aFW"
      },
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 5: Complete the routine below that divides the randomly shuffled data into multiple minibatches of size 500 and use the train function above to run gradient descent on those minibatches. Within each step, you should make a complete pass through the dataset. (10 pts) </b></h6></font> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHEgstAJMqrA"
      },
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 6: Get predictions on your test data set and report the MSE loss on the test data.  (5 pts) </b></h6></font> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNNF9Sf9lwAz",
        "outputId": "4d8ccafb-c170-461c-da1b-0ae24abdde6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=1.3202984>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions_test = model(X_test)\n",
        "mse(predictions_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO5l34GHZrDQ"
      },
      "source": [
        "# Multiclass Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfaxLYw4XDp-"
      },
      "source": [
        "Next, you will train the two layer neural network to do multiclass classification on digits dataset. Digits data is similar to MNIST  but has even smaller pixel. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJJii4cQxn_9"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "X,y_int = load_digits(return_X_y=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqD6t8NRXXRd"
      },
      "source": [
        "In HW2, you manually created an one hot encoding of the multiclass targets. We can use a function from keras, which is a high level deep learning API thats works pretty well with tensorflow. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjf5hOJny-CH"
      },
      "outputs": [],
      "source": [
        "from keras.utils.np_utils import to_categorical   \n",
        "y_one_hot = to_categorical(y_int, num_classes=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98zNQ6TkYMbV"
      },
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 7: Fill in the input dimension and output dimension of the two layer neural network appropriate for this dataset. (5 pts) </b></h6></font> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziX9ZaDUYTd1"
      },
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 8: Define  tensorflow variables for  weights W1, b1, W2, and b2. Then, initialize both biases b1 and b2 to be 0 and initialize W1 and W2 by picking values uniformly at random from the interval [0, 0.1].  (5 pts) </b></h6></font> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WF4RjiNZxO_"
      },
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 9: Complete the classifier function below to define a two layer neural network that outputs class probabilities for each class. Here, inputs is a matrix of shape $n \\times d$, where $i^{th}$ row of the inputs matrix contains $x_i^{\\intercal}$. (10 pts) </b></h6></font> Hint: use [tf.softmax.nn()](https://www.tensorflow.org/api_docs/python/tf/nn/softmax) function to compute softmax class probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Tub82FUbLkD"
      },
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 10: Complete the  function below to compute cross entropy loss given one hot encoding of targets and predictions with class probabilities for each prediction. (10 pts) </b></h6></font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHX2NPkw1NWr"
      },
      "outputs": [],
      "source": [
        "def cross_entropy(predictions, targets):\n",
        "  cr_entr = tf.reduce_sum((-1*tf.math.log(predictions))* targets, axis =1)\n",
        "  return tf.reduce_mean(cr_entr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EHBjhOnbpmM"
      },
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 11: Complete the function below to train the neural network classifier using gradient descent.  (5 pts) </b></h6></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHU_8kZ8cBlW"
      },
      "source": [
        "Train the model using 500 GD iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hVr6AALcRMB"
      },
      "source": [
        "Suppose, given the vector of class probabilities, you output the label with the highest class probability as your label. As an evaluation of our model, we want to compute the number of mistakes that the model makes. For instance, if $y$ is the true label and $\\widehat{y}$ is the prediction of the model, we will evaluate our model on this point with 0-1 loss\n",
        "$$\\mathbb{1}( \\widehat{y} \\neq y ).$$\n",
        "Over $n$ points, we will compute the mean 0-1 loss, \n",
        "$$\\frac{1}{n}\\sum_{i=1}^n \\mathbb{1}( \\widehat{y}_i \\neq y_i ).$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_yAWkZqeV3-"
      },
      "source": [
        "<font color=\"#de3023\"><h6><b>Question 11: Compute the mean 0-1 loss of your classifier on the test data. (5pts) </b></h6></font>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
